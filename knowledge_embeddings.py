# knowledge_embeddings.py

import os
import json
import numpy as np
import faiss
import sys
from openai import OpenAI

sys.path.append(os.path.dirname(os.path.abspath(__file__)) + "/..")

from config import (
    OPENAI_API_KEY,
    KNOWLEDGE_CHUNKS_PATH,
    KNOWLEDGE_INDEX_PATH,
    CONDITIONS_PATH,
    TOP_K_RESULTS
)

client = OpenAI(api_key=OPENAI_API_KEY)

# --- –†–æ–∑–±–∏—Ç—Ç—è —Ç–µ–∫—Å—Ç—É –Ω–∞ —á–∞–Ω–∫–∏ --- #
def split_into_chunks(text):
    paragraphs = text.split("\n\n")
    return [p.strip() for p in paragraphs if p.strip()]

# --- –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É –ø–µ—Ä–µ–¥ –µ–º–±–µ–¥—ñ–Ω–≥–æ–º --- #
def is_valid_text(t):
    return isinstance(t, str) and t.strip() and len(t.encode("utf-8")) <= 20000

# --- –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤ –¥–ª—è —Å–ø–∏—Å–∫—É —Ç–µ–∫—Å—Ç—ñ–≤ --- #
def embed_texts(texts):
    if not texts:
        return []

    embeddings = []
    batch_size = 100

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        try:
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=batch
            )
            for item in response.data:
                vec = np.array(item.embedding, dtype=np.float32)
                if vec.shape[0] == 1536:
                    embeddings.append(vec)
                else:
                    print("‚ö†Ô∏è –ù–µ–≤—ñ—Ä–Ω–∞ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –µ–º–±–µ–¥—ñ–Ω–≥–∞ ‚Äî –∑–∞–ø–∏—Å–∞–Ω–æ –Ω—É–ª—å–æ–≤–∏–π –≤–µ–∫—Ç–æ—Ä.")
                    embeddings.append(np.zeros((1536,), dtype=np.float32))
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤ embed_texts: {e}")
            embeddings.extend([np.zeros((1536,), dtype=np.float32)] * len(batch))

    return embeddings

# --- –ï–º–±–µ–¥—ñ–Ω–≥ –æ–¥–Ω–æ–≥–æ –∑–∞–ø–∏—Ç—É --- #
def embed_text(text):
    result = embed_texts([text])
    return result[0] if result else np.zeros((1536,), dtype=np.float32)

# --- –ü–æ—à—É–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∑–Ω–∞–Ω–Ω—è --- #
def search_combined_context(query):
    try:
        q_vec = embed_text(query)
        index = faiss.read_index(KNOWLEDGE_INDEX_PATH)

        D, I = index.search(np.array([q_vec]), TOP_K_RESULTS)

        with open(KNOWLEDGE_CHUNKS_PATH, "r", encoding="utf-8") as f:
            chunks = json.load(f)

        return [chunks[i] for i in I[0] if i < len(chunks)]
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –ø–æ—à—É–∫—É –∑–Ω–∞–Ω—å: {e}")
        return []

# --- –ü–æ–±—É–¥–æ–≤–∞ —ñ–Ω–¥–µ–∫—Å—É –∑–Ω–∞–Ω—å --- #
def main():
    print("üìö –ü–æ–±—É–¥–æ–≤–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —ñ–Ω–¥–µ–∫—Å—É –¥–ª—è CONDITIONS...")

    if not os.path.exists(CONDITIONS_PATH):
        print(f"‚ùå –§–∞–π–ª {CONDITIONS_PATH} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return

    with open(CONDITIONS_PATH, "r", encoding="utf-8") as f:
        raw = f.read().strip()

    if not raw:
        print("‚ö†Ô∏è –§–∞–π–ª —É–º–æ–≤ –ø–æ—Ä–æ–∂–Ω—ñ–π ‚Äî –Ω–µ–º–∞—î —â–æ –æ–±—Ä–æ–±–ª—è—Ç–∏.")
        return

    chunks = split_into_chunks(raw)
    valid_chunks = [c for c in chunks if is_valid_text(c)]

    if not valid_chunks:
        print("‚ö†Ô∏è –ù–µ–º–∞—î –≤–∞–ª—ñ–¥–Ω–∏—Ö —á–∞–Ω–∫—ñ–≤ –¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó.")
        return

    embeddings = embed_texts(valid_chunks)
    if not embeddings:
        print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –µ–º–±–µ–¥—ñ–Ω–≥–∏.")
        return

    dim = len(embeddings[0])
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(embeddings))

    enriched_chunks = [{"text": chunk} for chunk in valid_chunks]
    with open(KNOWLEDGE_CHUNKS_PATH, "w", encoding="utf-8") as f:
        json.dump(enriched_chunks, f, ensure_ascii=False, indent=2)

    faiss.write_index(index, KNOWLEDGE_INDEX_PATH)

    print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(valid_chunks)} –∑–Ω–∞–Ω—å —É {KNOWLEDGE_INDEX_PATH}")

# --- –¢–æ—á–∫–∞ –≤—Ö–æ–¥—É --- #
if __name__ == "__main__":
    main()
