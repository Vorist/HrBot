# build_knowledge_chunks.py

import os
import sys
import json
import numpy as np
import faiss
from openai import OpenAI

sys.path.append(os.path.dirname(os.path.abspath(__file__)) + "/..")

from config import (
    CONDITIONS_PATH,
    FREQUENT_QUESTIONS_PATH,
    KNOWLEDGE_CHUNKS_PATH,
    KNOWLEDGE_INDEX_PATH,
    OPENAI_API_KEY,
)

client = OpenAI(api_key=OPENAI_API_KEY)

# === üîê –ë–µ–∑–ø–µ—á–Ω–∏–π print (–¥–ª—è emoji —ñ —é–Ω—ñ–∫–æ–¥—É) ===
def safe_print(message):
    try:
        print(message)
    except UnicodeEncodeError:
        print(message.encode("ascii", "ignore").decode())


# === üìö –†–æ–∑–±–∏—Ç—Ç—è –Ω–∞ —á–∞–Ω–∫–∏ ===
def split_into_chunks(text, max_tokens=500):
    paragraphs = text.split("\n\n")
    chunks = []
    current = ""
    for para in paragraphs:
        if len(current + para) > max_tokens:
            chunks.append(current.strip())
            current = para + "\n\n"
        else:
            current += para + "\n\n"
    if current:
        chunks.append(current.strip())
    return chunks


# === üßΩ –§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É ===
def format_text(text):
    return text.replace("\n\n", "\n").strip()


# === üß† –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤ ===
def embed_texts(texts):
    if not texts:
        return []
    try:
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        return [np.array(obj.embedding, dtype=np.float32) for obj in response.data]
    except Exception as e:
        safe_print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤: {e}")
        return [np.zeros((1536,), dtype=np.float32) for _ in texts]


# === üìÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —É–º–æ–≤ ===
def load_conditions():
    if not os.path.exists(CONDITIONS_PATH):
        safe_print(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª: {CONDITIONS_PATH}")
        return []
    try:
        with open(CONDITIONS_PATH, "r", encoding="utf-8") as f:
            text = f.read().strip()
        return [{"type": "condition", "text": chunk} for chunk in split_into_chunks(text)]
    except Exception as e:
        safe_print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è —É–º–æ–≤: {e}")
        return []


# === ‚ùì –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —á–∞—Å—Ç–∏—Ö –ø–∏—Ç–∞–Ω—å ===
def load_frequent_questions():
    if not os.path.exists(FREQUENT_QUESTIONS_PATH):
        safe_print(f"‚ö†Ô∏è –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª: {FREQUENT_QUESTIONS_PATH}")
        return []
    try:
        with open(FREQUENT_QUESTIONS_PATH, "r", encoding="utf-8") as f:
            raw = json.load(f)

        if isinstance(raw, dict):
            cleaned = {
                str(k): int(v) if isinstance(v, int) or str(v).isdigit() else 0
                for k, v in raw.items()
            }
            sorted_items = sorted(cleaned.items(), key=lambda x: x[1], reverse=True)
            data = [{"question": q, "answer": ""} for q, _ in sorted_items]
            with open(FREQUENT_QUESTIONS_PATH, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        elif isinstance(raw, list):
            data = raw
        else:
            safe_print("‚ö†Ô∏è –§–æ—Ä–º–∞—Ç frequent_questions.json –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è.")
            return []

        return [{
            "type": "faq",
            "text": format_text(f"‚ùì {item['question']}\nüí¨ {item.get('answer', '')}")
        } for item in data if "question" in item]

    except Exception as e:
        safe_print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ {FREQUENT_QUESTIONS_PATH}: {e}")
        return []


# === üîß –ü–æ–±—É–¥–æ–≤–∞ –∑–Ω–∞–Ω—å ===
def build_knowledge_chunks():
    chunks = load_conditions() + load_frequent_questions()
    safe_print(f"üì• –û—Ç—Ä–∏–º–∞–Ω–æ {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤ –∑–Ω–∞–Ω—å.")

    texts = [item["text"] for item in chunks]
    embeddings = embed_texts(texts)

    valid_chunks = []
    for chunk, vector in zip(chunks, embeddings):
        if isinstance(vector, np.ndarray) and vector.shape[0] == 1536:
            valid_chunks.append({**chunk, "vector": vector.tolist()})
        else:
            safe_print("‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ chunk: –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–∞ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –µ–º–±–µ–¥—ñ–Ω–≥—É")

    if not valid_chunks:
        safe_print("‚ùå –ñ–æ–¥–Ω–æ–≥–æ –≤–∞–ª—ñ–¥–Ω–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—É –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è.")
        return

    os.makedirs(os.path.dirname(KNOWLEDGE_CHUNKS_PATH), exist_ok=True)
    with open(KNOWLEDGE_CHUNKS_PATH, "w", encoding="utf-8") as f:
        json.dump(valid_chunks, f, ensure_ascii=False, indent=2)

    dim = len(valid_chunks[0]["vector"])
    index = faiss.IndexFlatL2(dim)
    index.add(np.array([chunk["vector"] for chunk in valid_chunks], dtype=np.float32))
    faiss.write_index(index, KNOWLEDGE_INDEX_PATH)

    safe_print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(valid_chunks)} –∑–Ω–∞–Ω—å ‚Üí {KNOWLEDGE_INDEX_PATH}")


# === ‚ñ∂Ô∏è –¢–æ—á–∫–∞ –≤—Ö–æ–¥—É ===
if __name__ == "__main__":
    build_knowledge_chunks()
