# build_embeddings.py

import os
import json
import numpy as np
import faiss
from openai import OpenAI
from config import (
    OPENAI_API_KEY,
    CONDITIONS_PATH,
    KNOWLEDGE_CHUNKS_PATH,
    KNOWLEDGE_INDEX_PATH
)

# === –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è OpenAI –∫–ª—ñ—î–Ω—Ç–∞ ===
client = OpenAI(api_key=OPENAI_API_KEY)

# === –†–æ–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç—É –Ω–∞ —á–∞–Ω–∫–∏ ===
def split_into_chunks(text, max_chars=1000):
    paragraphs = text.split("\n\n")
    chunks, current = [], ""

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue

        if len(current + para) > max_chars:
            if current:
                chunks.append(current.strip())
            current = para + "\n\n"
        else:
            current += para + "\n\n"

    if current.strip():
        chunks.append(current.strip())

    return chunks

# === –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤ ===
def embed_texts(texts):
    if not texts:
        return []

    embeddings = []
    try:
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        for item in response.data:
            vec = np.array(item.embedding, dtype=np.float32)
            if vec.shape[0] == 1536:
                embeddings.append(vec)
            else:
                print("‚ö†Ô∏è –ù–µ–∫–æ—Ä–µ–∫—Ç–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –µ–º–±–µ–¥—ñ–Ω–≥–∞, –¥–æ–¥–∞—é zero-–≤–µ–∫—Ç–æ—Ä.")
                embeddings.append(np.zeros((1536,), dtype=np.float32))
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤: {e}")
        embeddings = [np.zeros((1536,), dtype=np.float32) for _ in texts]

    return embeddings

# === –û—Å–Ω–æ–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞ ===
def main():
    print("üìö –°—Ç–≤–æ—Ä–µ–Ω–Ω—è embedding-—ñ–Ω–¥–µ–∫—Å—É –∑ —Ñ–∞–π–ª—É —É–º–æ–≤...")

    if not os.path.exists(CONDITIONS_PATH):
        print(f"‚ùå –§–∞–π–ª —É–º–æ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {CONDITIONS_PATH}")
        return

    with open(CONDITIONS_PATH, "r", encoding="utf-8") as f:
        raw_text = f.read().strip()

    if not raw_text:
        print("‚ö†Ô∏è –§–∞–π–ª —É–º–æ–≤ –ø–æ—Ä–æ–∂–Ω—ñ–π!")
        return

    chunks = split_into_chunks(raw_text)
    if not chunks:
        print("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ –≤–∞–ª—ñ–¥–Ω–æ–≥–æ —á–∞–Ω–∫—É.")
        return

    embeddings = embed_texts(chunks)
    if not embeddings or any(e.shape[0] == 0 for e in embeddings):
        print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –≤–∞–ª—ñ–¥–Ω—ñ –µ–º–±–µ–¥—ñ–Ω–≥–∏.")
        return

    index = faiss.IndexFlatL2(embeddings[0].shape[0])
    index.add(np.array(embeddings, dtype=np.float32))

    with open(KNOWLEDGE_CHUNKS_PATH, "w", encoding="utf-8") as f:
        json.dump([{"text": chunk} for chunk in chunks], f, ensure_ascii=False, indent=2)

    faiss.write_index(index, KNOWLEDGE_INDEX_PATH)

    print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(chunks)} –∑–Ω–∞–Ω—å —É {KNOWLEDGE_INDEX_PATH}")

# === –ó–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    main()
